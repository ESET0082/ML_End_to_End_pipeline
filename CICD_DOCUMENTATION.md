# CI/CD Pipeline Documentation

## Overview

This CI/CD pipeline automates testing, building, and deployment of the Meter Consumption Prediction ML system.

**Pipeline Stages:**
1. âœ… Code Quality & Linting
2. ğŸ”’ Security Scanning
3. ğŸ§ª Unit Tests
4. ğŸ³ Docker Image Build
5. ğŸ“Š Data Validation
6. ğŸ”— Integration Tests
7. âš¡ Performance Benchmarks
8. ğŸš€ Deploy to Staging
9. ğŸŒ Deploy to Production
10. ğŸ“¢ Notifications

---

## Pipeline Triggers

### Automatic Triggers
```yaml
# On every push to main or develop
on:
  push:
    branches: [ main, develop ]

# On every pull request
on:
  pull_request:
    branches: [ main, develop ]

# Weekly scheduled run
on:
  schedule:
    - cron: '0 2 * * 0'  # Every Sunday at 2 AM
```

---

## Stage 1: Code Quality & Linting

**Tools Used:**
- **Black**: Code formatting
- **isort**: Import sorting
- **Flake8**: Style guide enforcement
- **Pylint**: Code analysis

**Checks:**
```
âœ“ Code formatting (Black)
âœ“ Import ordering (isort)
âœ“ Style violations (Flake8)
âœ“ Code quality score (Pylint â‰¥ 7.0)
```

**Fix Issues Locally:**
```bash
# Format code
black src/

# Sort imports
isort src/

# Check linting
flake8 src/
pylint src/
```

---

## Stage 2: Security Scanning

**Tools Used:**
- **Bandit**: Security issue detection
- **Safety**: Dependency vulnerability scanning

**Checks:**
```
âœ“ Hardcoded passwords/secrets
âœ“ SQL injection vulnerabilities
âœ“ Insecure cryptography
âœ“ Known CVEs in dependencies
```

**Artifacts Generated:**
- `bandit-report.json` - Security issues
- `safety-report.json` - Dependency vulnerabilities

---

## Stage 3: Unit Tests

**Components Tested:**
- Data ingestion (`src/data/ingestion.py`)
- Feature engineering (`src/data/create_datasets.py`)
- Model training (`src/models/train.py`)
- Model inference (`src/models/inference.py`)
- API endpoints (`src/api/server.py`)

**Coverage Requirements:**
- Minimum 80% code coverage
- All critical paths tested
- Error handling validated

**Run Locally:**
```bash
pytest tests/ -v --cov=src --cov-report=html
```

**Test Database:**
- PostgreSQL 15 (auto-provisioned)
- Isolated test database
- Auto-cleanup after tests

---

## Stage 4: Docker Image Build

**Images Built:**
1. **Airflow** (`docker/airflow/Dockerfile`)
   - DAG orchestration
   - Scheduler & webserver

2. **MLflow** (`docker/mlflow/Dockerfile`)
   - Model tracking server
   - Experiment management

3. **Model API** (`docker/model_api/Dockerfile`)
   - FastAPI server
   - Real-time predictions

**Registry:** GitHub Container Registry (ghcr.io)

**Tagging Strategy:**
```
- Branch: main-latest
- Semver: v1.2.3
- SHA: abc1234567
```

**Build Optimization:**
- Docker layer caching
- Multi-stage builds
- Minimal image sizes

---

## Stage 5: Data Validation

**Validation Rules:**
```
âœ“ CSV files exist and are readable
âœ“ Data schema matches expectations
âœ“ No empty datasets
âœ“ Data types are correct
âœ“ Required columns present
```

**Checks:**
- File format validation
- Schema consistency
- Data quality metrics

---

## Stage 6: Integration Tests

**Test Scope:**
- Data pipeline imports
- Training pipeline functionality
- Inference pipeline functionality
- API schema validation
- Cross-component communication

**Example Tests:**
```python
# Data pipeline
from data.create_datasets import load_data, create_features
load_data()  # Should succeed

# Model pipeline
from models.train import train_logistic_regression
train_logistic_regression()  # Should complete

# API schema
from api.server import MeterFeatures
meter = MeterFeatures(...)  # Should validate
```

---

## Stage 7: Performance Benchmarks

**Benchmarks Measured:**
- Model loading time
- Single prediction latency (< 5ms target)
- Feature engineering speed
- API response time (< 100ms target)

**Success Criteria:**
```
âœ“ Single prediction: < 5ms
âœ“ Batch predictions (100): < 500ms
âœ“ Model loading: < 1s
âœ“ API response: < 100ms
```

---

## Stage 8: Deploy to Staging

**Triggers:**
- Push to `develop` branch

**Actions:**
```
1. âœ“ Docker images pushed to registry
2. âœ“ Deploy scripts executed
3. âœ“ Environment variables configured
4. âœ“ Health checks performed
5. âœ“ Smoke tests run
```

**Smoke Tests:**
- API endpoints responding
- Database connections healthy
- All services running

**Rollback:**
- Automatic if health checks fail
- Manual rollback available

---

## Stage 9: Deploy to Production

**Triggers:**
- Push to `main` branch
- All tests passed
- Staging deployment successful

**Deployment Strategy: Blue-Green**
```
1. âœ“ Verify production readiness
2. âœ“ Deploy new version (Green)
3. âœ“ Run health checks
4. âœ“ Gradually shift traffic
5. âœ“ Monitor metrics
6. âœ“ Rollback if needed
```

**Production Requirements:**
- All unit tests passing (100%)
- Integration tests successful
- Performance benchmarks met
- Security scan clean
- Code quality score â‰¥ 8.0

**Monitoring Active:**
- Error rates tracked
- Performance metrics logged
- Alerts configured
- Audit trail maintained

---

## Stage 10: Notifications

**Success Notification:**
```
âœ… CI/CD Pipeline Successful
- All checks passed
- Docker images built
- Tests: 95/95 passed
- Coverage: 88%
```

**Failure Notification:**
```
âŒ CI/CD Pipeline Failed
- Stage: Unit Tests
- Error: Test timeout
- Log: See GitHub Actions
```

**Channels:**
- Slack (if configured)
- GitHub notifications
- Email (optional)
- Team dashboard

---

## GitHub Secrets Required

Add these secrets to GitHub repository settings:

```
SLACK_WEBHOOK_URL          # For Slack notifications
REGISTRY_USERNAME          # Container registry username
REGISTRY_PASSWORD          # Container registry token
DATABASE_URL               # Test database URL
MLFLOW_TRACKING_URI        # MLflow server URL
```

---

## Pipeline Workflow Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Push/PR       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  CODE QUALITY & LINTING     â”‚ (5 min)
    â”‚  â€¢ Black, isort, Flake8     â”‚
    â”‚  â€¢ Pylint check             â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”˜
         â”‚                        â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   SECURITY  â”‚   â”‚  UNIT TESTS       â”‚ (10 min)
    â”‚   SCANNING  â”‚   â”‚  â€¢ Coverage: 80%+ â”‚
    â”‚ â€¢ Bandit    â”‚   â”‚  â€¢ Pytest         â”‚
    â”‚ â€¢ Safety    â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜           â”‚
             â”‚                â”‚
         â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”
         â”‚  DATA VALIDATION      â”‚ (2 min)
         â”‚  â€¢ Schema checks      â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
              â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚ BUILD DOCKER       â”‚ (15 min)
              â”‚ â€¢ 3 images         â”‚
              â”‚ â€¢ Registry push    â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚ INTEGRATION TESTS   â”‚ (5 min)
            â”‚ â€¢ Pipeline tests    â”‚
            â”‚ â€¢ API validation    â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚ PERFORMANCE       â”‚ (3 min)
            â”‚ BENCHMARKS        â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚  DEPLOY STAGING        â”‚ (5 min)
          â”‚ (if develop branch)    â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚ DEPLOY PRODUCTION      â”‚ (10 min)
          â”‚ (if main branch)       â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚ NOTIFICATIONS & REPORTSâ”‚
          â”‚ â€¢ Slack/Email          â”‚
          â”‚ â€¢ Test artifacts       â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Total Pipeline Duration:** ~50-60 minutes

---

## Local Testing Before Push

**Run these commands locally to avoid CI failures:**

```bash
# Code quality
black src/
isort src/
flake8 src/

# Security
bandit -r src/
safety check

# Tests
pytest tests/ -v --cov=src

# Docker build (optional)
docker build -f docker/airflow/Dockerfile -t airflow:test .
```

---

## Troubleshooting

### Test Failure
1. Check GitHub Actions logs
2. Review error message
3. Run locally: `pytest tests/ -v`
4. Fix and commit
5. Push to trigger pipeline again

### Performance Degradation
1. Check performance benchmark results
2. Profile code locally
3. Optimize critical paths
4. Verify database connections

### Deployment Failure
1. Check staging deployment first
2. Verify all prerequisites passed
3. Check error logs in Actions
4. Manual rollback if needed

---

## Best Practices

âœ… **Commit Guidelines:**
- Run local tests before pushing
- Write meaningful commit messages
- Reference issue numbers

âœ… **Pull Requests:**
- Create PR for features
- Wait for all checks to pass
- Request code review
- Merge only after approval

âœ… **Branching:**
- `main` = Production ready
- `develop` = Staging/testing
- `feature/*` = New features

âœ… **Secrets Management:**
- Never commit secrets
- Use GitHub secrets
- Rotate credentials regularly

---

## Monitoring & Maintenance

**Weekly Review:**
- Check pipeline success rate
- Monitor performance trends
- Review security scan results

**Monthly Maintenance:**
- Update dependencies
- Review test coverage
- Optimize pipeline duration

**Quarterly:**
- Full security audit
- Performance baseline
- Process improvement review

